{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8166514,"sourceType":"datasetVersion","datasetId":4826829},{"sourceId":8262224,"sourceType":"datasetVersion","datasetId":4904105}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **MS LESION SEGMENTATION TESTING SegResNet** ","metadata":{}},{"cell_type":"markdown","source":"Computation of performance metrics (nDSC, lesion F1 score, nDSC R-AUC) for a model.","metadata":{}},{"cell_type":"markdown","source":"## Install Libraries ","metadata":{}},{"cell_type":"code","source":"!pip install monai==0.9.0","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:16:41.767119Z","iopub.execute_input":"2024-05-09T15:16:41.767585Z","iopub.status.idle":"2024-05-09T15:16:58.820043Z","shell.execute_reply.started":"2024-05-09T15:16:41.767551Z","shell.execute_reply":"2024-05-09T15:16:58.818296Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting monai==0.9.0\n  Using cached monai-0.9.0-202206131636-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (2.1.2+cpu)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->monai==0.9.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->monai==0.9.0) (1.3.0)\nUsing cached monai-0.9.0-202206131636-py3-none-any.whl (939 kB)\nInstalling collected packages: monai\nSuccessfully installed monai-0.9.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Libraries Import","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nimport re\nfrom joblib import Parallel\nfrom monai.inferers import sliding_window_inference\nfrom monai.networks.nets import SegResNet\nfrom monai.data import CacheDataset, DataLoader\nfrom monai.transforms import (\n    AddChanneld, Compose, LoadImaged, RandCropByPosNegLabeld,\n    Spacingd, ToTensord, NormalizeIntensityd, RandFlipd,\n    RandRotate90d, RandShiftIntensityd, RandAffined, RandSpatialCropd,\n    RandScaleIntensityd)\nfrom glob import glob\nfrom scipy import ndimage\nfrom functools import partial\nfrom collections import Counter\nfrom joblib import Parallel, delayed\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:16:58.823032Z","iopub.execute_input":"2024-05-09T15:16:58.823492Z","iopub.status.idle":"2024-05-09T15:17:46.730739Z","shell.execute_reply.started":"2024-05-09T15:16:58.823453Z","shell.execute_reply":"2024-05-09T15:17:46.729340Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-09 15:17:35.343384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-09 15:17:35.343549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-09 15:17:35.504972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup Functions","metadata":{}},{"cell_type":"code","source":"def get_default_device():\n    \"\"\" Set device \"\"\"\n    if torch.cuda.is_available():\n        print(\"Got CUDA!\")\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.732889Z","iopub.execute_input":"2024-05-09T15:17:46.734270Z","iopub.status.idle":"2024-05-09T15:17:46.742444Z","shell.execute_reply.started":"2024-05-09T15:17:46.734227Z","shell.execute_reply":"2024-05-09T15:17:46.740629Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Data Load","metadata":{}},{"cell_type":"code","source":"def remove_connected_components(segmentation, l_min=9):\n    \"\"\"\n    Remove all lesions with less or equal amount of voxels than `l_min` from a \n    binary segmentation mask `segmentation`.\n    Args:\n      segmentation: `numpy.ndarray` of shape [H, W, D], with a binary lesions segmentation mask.\n      l_min:  `int`, minimal amount of voxels in a lesion.\n    Returns:\n      Binary lesion segmentation mask (`numpy.ndarray` of shape [H, W, D])\n      only with connected components that have more than `l_min` voxels.\n    \"\"\"\n    labeled_seg, num_labels = ndimage.label(segmentation)\n    label_list = np.unique(labeled_seg)\n    num_elements_by_lesion = ndimage.labeled_comprehension(segmentation, labeled_seg, label_list, np.sum, float, 0)\n\n    seg2 = np.zeros_like(segmentation)\n    for i_el, n_el in enumerate(num_elements_by_lesion):\n        if n_el > l_min:\n            current_voxels = np.stack(np.where(labeled_seg == i_el), axis=1)\n            seg2[current_voxels[:, 0],\n                 current_voxels[:, 1],\n                 current_voxels[:, 2]] = 1\n    return seg2","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.745080Z","iopub.execute_input":"2024-05-09T15:17:46.745785Z","iopub.status.idle":"2024-05-09T15:17:46.796209Z","shell.execute_reply.started":"2024-05-09T15:17:46.745743Z","shell.execute_reply":"2024-05-09T15:17:46.794752Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_val_transforms(keys=[\"image\", \"label\"], image_keys=[\"image\"]):\n    \"\"\" Get transforms for testing on FLAIR images and ground truth:\n    - Loads 3D images and masks from Nifti file\n    - Adds channel dimention\n    - Applies intensity normalisation to scans\n    - Converts to torch.Tensor()\n    \"\"\"\n    return Compose(\n        [\n            LoadImaged(keys=keys),\n            AddChanneld(keys=keys),\n            NormalizeIntensityd(keys=image_keys, nonzero=True),\n            ToTensord(keys=keys),\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.800022Z","iopub.execute_input":"2024-05-09T15:17:46.801231Z","iopub.status.idle":"2024-05-09T15:17:46.814168Z","shell.execute_reply.started":"2024-05-09T15:17:46.801179Z","shell.execute_reply":"2024-05-09T15:17:46.812956Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_val_dataloader(flair_path, gts_path, num_workers, cache_rate=0.1, bm_path=None):\n    \"\"\"\n    Get dataloader for validation and testing. Either with or without brain masks.\n\n    Args:\n      flair_path: `str`, path to directory with FLAIR images.\n      gts_path:  `str`, path to directory with ground truth lesion segmentation \n                    binary masks images.\n      num_workers:  `int`,  number of worker threads to use for parallel processing\n                    of images\n      cache_rate:  `float` in (0.0, 1.0], percentage of cached data in total.\n      bm_path:   `None|str`. If `str`, then defines path to directory with\n                 brain masks. If `None`, dataloader does not return brain masks. \n    Returns:\n      monai.data.DataLoader() class object.\n    \"\"\"\n    flair = sorted(glob(os.path.join(flair_path, \"*FLAIR_isovox.nii\")),\n                   key=lambda i: int(re.sub('\\D', '', i)))  # Collect all flair images sorted\n    segs = sorted(glob(os.path.join(gts_path, \"*_isovox.nii\")),\n                  key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding ground truths\n\n    if bm_path is not None:\n        bms = sorted(glob(os.path.join(bm_path, \"*isovox_fg_mask.nii\")),\n                     key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding brain masks\n\n        assert len(flair) == len(segs) == len(bms), f\"Some files must be missing: {[len(flair), len(segs), len(bms)]}\"\n\n        files = [\n            {\"image\": fl, \"label\": seg, \"brain_mask\": bm} for fl, seg, bm\n            in zip(flair, segs, bms)\n        ]\n\n        val_transforms = get_val_transforms(keys=[\"image\", \"label\", \"brain_mask\"])\n    else:\n        assert len(flair) == len(segs), f\"Some files must be missing: {[len(flair), len(segs)]}\"\n\n        files = [{\"image\": fl, \"label\": seg} for fl, seg in zip(flair, segs)]\n\n        val_transforms = get_val_transforms()\n\n    print(\"Number of validation files:\", len(files))\n\n    ds = CacheDataset(data=files, transform=val_transforms,\n                      cache_rate=cache_rate, num_workers=num_workers)\n    return DataLoader(ds, batch_size=1, shuffle=False,\n                      num_workers=num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.816108Z","iopub.execute_input":"2024-05-09T15:17:46.816948Z","iopub.status.idle":"2024-05-09T15:17:46.835411Z","shell.execute_reply.started":"2024-05-09T15:17:46.816909Z","shell.execute_reply":"2024-05-09T15:17:46.834108Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"def dice_norm_metric(ground_truth, predictions):\n    \"\"\"\n    Compute Normalised Dice Coefficient (nDSC), \n    False positive rate (FPR),\n    False negative rate (FNR) for a single example.\n    \n    Args:\n      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n                     with shape [H, W, D].\n      predictions:  `numpy.ndarray`, binary segmentation predictions,\n                     with shape [H, W, D].\n    Returns:\n      Normalised dice coefficient (`float` in [0.0, 1.0]),\n      False positive rate (`float` in [0.0, 1.0]),\n      False negative rate (`float` in [0.0, 1.0]),\n      between `ground_truth` and `predictions`.\n    \"\"\"\n\n    # Reference for normalized DSC\n    r = 0.001\n    # Cast to float32 type\n    gt = ground_truth.astype(\"float32\")\n    seg = predictions.astype(\"float32\")\n    im_sum = np.sum(seg) + np.sum(gt)\n    if im_sum == 0:\n        return 1.0\n    else:\n        if np.sum(gt) == 0:\n            k = 1.0\n        else:\n            k = (1 - r) * np.sum(gt) / (r * (len(gt.flatten()) - np.sum(gt)))\n        tp = np.sum(seg[gt == 1])\n        fp = np.sum(seg[gt == 0])\n        fn = np.sum(gt[seg == 0])\n        fp_scaled = k * fp\n        dsc_norm = 2. * tp / (fp_scaled + 2. * tp + fn)\n        return dsc_norm","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.837778Z","iopub.execute_input":"2024-05-09T15:17:46.838728Z","iopub.status.idle":"2024-05-09T15:17:46.857348Z","shell.execute_reply.started":"2024-05-09T15:17:46.838662Z","shell.execute_reply":"2024-05-09T15:17:46.855660Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def ndsc_aac_metric(ground_truth, predictions, uncertainties, parallel_backend=None):\n    \"\"\"\n    Compute area above Normalised Dice Coefficient (nDSC) retention curve for \n    one subject. `ground_truth`, `predictions`, `uncertainties` - are flattened \n    arrays of correponding 3D maps within the foreground mask only.\n    \n    Args:\n      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n                     with shape [H * W * D]. \n      predictions:  `numpy.ndarray`, binary segmentation predictions,\n                     with shape [H * W * D].\n      uncertainties:  `numpy.ndarray`, voxel-wise uncertainties,\n                     with shape [H * W * D].\n      parallel_backend: `joblib.Parallel`, for parallel computation\n                     for different retention fractions.\n    Returns:\n      nDSC R-AAC (`float` in [0.0, 1.0]).\n    \"\"\"\n\n    def compute_dice_norm(frac_, preds_, gts_, N_):\n        pos = int(N_ * frac_)\n        curr_preds = preds if pos == N_ else np.concatenate(\n            (preds_[:pos], gts_[pos:]))\n        return dice_norm_metric(gts_, curr_preds)\n\n    if parallel_backend is None:\n        parallel_backend = Parallel(n_jobs=1)\n\n    ordering = uncertainties.argsort()\n    gts = ground_truth[ordering].copy()\n    preds = predictions[ordering].copy()\n    N = len(gts)\n\n    # # Significant class imbalance means it is important to use logspacing between values\n    # # so that it is more granular for the higher retention fractions\n    fracs_retained = np.log(np.arange(200 + 1)[1:])\n    fracs_retained /= np.amax(fracs_retained)\n\n    process = partial(compute_dice_norm, preds_=preds, gts_=gts, N_=N)\n    dsc_norm_scores = np.asarray(\n        parallel_backend(delayed(process)(frac)\n                         for frac in fracs_retained)\n    )\n\n    return 1. - metrics.auc(fracs_retained, dsc_norm_scores)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.859278Z","iopub.execute_input":"2024-05-09T15:17:46.860289Z","iopub.status.idle":"2024-05-09T15:17:46.880707Z","shell.execute_reply.started":"2024-05-09T15:17:46.860250Z","shell.execute_reply":"2024-05-09T15:17:46.879330Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def intersection_over_union(mask1, mask2):\n    \"\"\"\n    Compute IoU for 2 binary masks.\n    \n    Args:\n      mask1: `numpy.ndarray`, binary mask.\n      mask2:  `numpy.ndarray`, binary mask of the same shape as `mask1`.\n    Returns:\n      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n    \"\"\"\n    return np.sum(mask1 * mask2) / np.sum(mask1 + mask2 - mask1 * mask2)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.882761Z","iopub.execute_input":"2024-05-09T15:17:46.884204Z","iopub.status.idle":"2024-05-09T15:17:46.902447Z","shell.execute_reply.started":"2024-05-09T15:17:46.884155Z","shell.execute_reply":"2024-05-09T15:17:46.900610Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def lesion_f1_score(ground_truth, predictions, IoU_threshold=0.25, parallel_backend=None):\n    \"\"\"\n    Compute lesion-scale F1 score.\n    \n    Args:\n      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n                     with shape [H, W, D].\n      predictions:  `numpy.ndarray`, binary segmentation predictions,\n                     with shape [H, W, D].\n      IoU_threshold: `float` in [0.0, 1.0], IoU threshold for max IoU between \n                     predicted and ground truth lesions to classify them as\n                     TP, FP or FN.\n      parallel_backend: `joblib.Parallel`, for parallel computation\n                     for different retention fractions.\n    Returns:\n      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n    \"\"\"\n\n    def get_tp_fp(label_pred, mask_multi_pred, mask_multi_gt):\n        mask_label_pred = (mask_multi_pred == label_pred).astype(int)\n        all_iou = [0.0]\n        # iterate only intersections\n        for int_label_gt in np.unique(mask_multi_gt * mask_label_pred):\n            if int_label_gt != 0.0:\n                mask_label_gt = (mask_multi_gt == int_label_gt).astype(int)\n                all_iou.append(intersection_over_union(\n                    mask_label_pred, mask_label_gt))\n        max_iou = max(all_iou)\n        if max_iou >= IoU_threshold:\n            return 'tp'\n        else:\n            return 'fp'\n\n    def get_fn(label_gt, mask_multi_pred, mask_multi_gt):\n        mask_label_gt = (mask_multi_gt == label_gt).astype(int)\n        all_iou = [0]\n        for int_label_pred in np.unique(mask_multi_pred * mask_label_gt):\n            if int_label_pred != 0.0:\n                mask_label_pred = (mask_multi_pred ==\n                                   int_label_pred).astype(int)\n                all_iou.append(intersection_over_union(\n                    mask_label_pred, mask_label_gt))\n        max_iou = max(all_iou)\n        if max_iou < IoU_threshold:\n            return 1\n        else:\n            return 0\n\n    mask_multi_pred_, n_les_pred = ndimage.label(predictions)\n    mask_multi_gt_, n_les_gt = ndimage.label(ground_truth)\n\n    if parallel_backend is None:\n        parallel_backend = Parallel(n_jobs=1)\n\n    process_fp_tp = partial(get_tp_fp, mask_multi_pred=mask_multi_pred_,\n                            mask_multi_gt=mask_multi_gt_)\n\n    tp_fp = parallel_backend(delayed(process_fp_tp)(label_pred)\n                             for label_pred in np.unique(mask_multi_pred_) if label_pred != 0)\n    counter = Counter(tp_fp)\n    tp = float(counter['tp'])\n    fp = float(counter['fp'])\n\n    process_fn = partial(get_fn, mask_multi_pred=mask_multi_pred_,\n                         mask_multi_gt=mask_multi_gt_)\n\n    fn = parallel_backend(delayed(process_fn)(label_gt)\n                          for label_gt in np.unique(mask_multi_gt_) if label_gt != 0)\n    fn = float(np.sum(fn))\n\n    f1 = 1.0 if tp + 0.5 * (fp + fn) == 0.0 else tp / (tp + 0.5 * (fp + fn))\n\n    return f1","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.904603Z","iopub.execute_input":"2024-05-09T15:17:46.905074Z","iopub.status.idle":"2024-05-09T15:17:46.924188Z","shell.execute_reply.started":"2024-05-09T15:17:46.905039Z","shell.execute_reply":"2024-05-09T15:17:46.922598Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Uncertainty","metadata":{}},{"cell_type":"code","source":"def renyi_entropy_of_expected(probs, alpha=0.8):\n    \"\"\"\n    Renyi entropy is a generalised version of Shannon - the two are equivalent for alpha=1\n    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n    \"\"\"\n    scale = 1. / (1. - alpha)\n    mean_probs = np.mean(probs, axis=0)\n    return scale * np.log( np.sum(mean_probs**alpha, axis=-1) )\n\ndef renyi_expected_entropy(probs, alpha=0.8):\n    \"\"\"\n    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n    \"\"\"\n    scale = 1. / (1. - alpha)\n    return np.mean( scale * np.log( np.sum(probs**alpha, axis=-1) ), axis=0)\n\n\ndef entropy_of_expected(probs, epsilon=1e-10):\n    \"\"\"\n    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n    \"\"\"\n    mean_probs = np.mean(probs, axis=0)\n    log_probs = -np.log(mean_probs + epsilon)\n    return np.sum(mean_probs * log_probs, axis=-1)\n\ndef expected_entropy(probs, epsilon=1e-10):\n    \"\"\"\n    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n    \"\"\"\n    log_probs = -np.log(probs + epsilon)\n    return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n\n\ndef ensemble_uncertainties_classification(probs, epsilon=1e-10):\n    \"\"\"\n    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n    :return: Dictionary of uncertainties\n    \"\"\"\n    mean_probs = np.mean(probs, axis=0)\n    mean_lprobs = np.mean(np.log(probs + epsilon), axis=0)\n    conf = np.max(mean_probs, axis=-1)\n\n    eoe = entropy_of_expected(probs, epsilon)\n    exe = expected_entropy(probs, epsilon)\n\n    mutual_info = eoe - exe\n\n    epkl = -np.sum(mean_probs * mean_lprobs, axis=-1) - exe\n\n    uncertainty = {'confidence': -1 * conf,\n                   'entropy_of_expected': eoe,\n                   'expected_entropy': exe,\n                   'mutual_information': mutual_info,\n                   'epkl': epkl,\n                   'reverse_mutual_information': epkl - mutual_info,\n                   }\n\n    return uncertainty","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.925540Z","iopub.execute_input":"2024-05-09T15:17:46.925970Z","iopub.status.idle":"2024-05-09T15:17:46.948522Z","shell.execute_reply.started":"2024-05-09T15:17:46.925937Z","shell.execute_reply":"2024-05-09T15:17:46.947294Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Testing Function","metadata":{}},{"cell_type":"code","source":"\"\"\"\nTesting function for UNET model.\nArgs:\n    Data Args:\n        path_test_data: `str`, path to directory with FLAIR images from test set.\n        path_test_gts: `str`, path to directory with ground truth lesion segmentation from test set.\n        path_test_bm: `str`, path to directory with brain masks from test set.\n    Model Args:\n        threshold: `float`, threshold for binary segmentation mask.\n        num_models: `int`, number of models to ensemble.\n        path_model: `str`, path to directory with trained models.\n        num_workers: `int`, number of worker threads to use for parallel processing of images.\n        n_jobs: `int`, number of jobs to run in parallel.\n\"\"\"\ndef testSegResNet(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 3, path_model = '', num_workers = 1, n_jobs = 1):\n    #Setting up the device\n    device = get_default_device()\n    torch.multiprocessing.set_sharing_strategy('file_system')\n    \n    #Initialise dataloaders\n    val_loader = get_val_dataloader(flair_path=path_test_data,\n                                    gts_path=path_test_gts,\n                                    num_workers=num_workers ,\n                                    bm_path=path_test_bm)\n    # Load trained models\n    K = num_models\n    models = []\n    for i in range(K):\n        models.append(SegResNet(\n            spatial_dims=3,\n            in_channels=1,\n            out_channels=2).to(device))\n\n    if(get_default_device() == torch.device('cpu')):\n        for i, model in enumerate(models):\n            model.load_state_dict(torch.load(os.path.join(path_model,\n                                                      \"Best_model_finetuning.pth\"),\n                                                      map_location=torch.device('cpu')))\n            model.eval()\n    else :\n        for i, model in enumerate(models):\n            model.load_state_dict(torch.load(os.path.join(path_model,\n                                                      \"Best_model_finetuning.pth\")))\n            model.eval()\n    \n    act = torch.nn.Softmax(dim=1)\n    th = threshold\n    roi_size = (96, 96, 96)\n    sw_batch_size = 4\n\n    ndsc, f1, ndsc_aac = [], [], []\n\n    #Evaluatioin loop\n    with Parallel(n_jobs= n_jobs) as parallel_backend:\n        with torch.no_grad():\n            for count, batch_data in enumerate(val_loader):\n                inputs, gt, brain_mask = (\n                    batch_data[\"image\"].to(device),\n                    batch_data[\"label\"].cpu().numpy(),\n                    batch_data[\"brain_mask\"].cpu().numpy()\n                )\n\n                # get ensemble predictions\n                all_outputs = []\n                for model in models:\n                    outputs = sliding_window_inference(inputs, roi_size,\n                                                       sw_batch_size, model,\n                                                       mode='gaussian')\n                    outputs = act(outputs).cpu().numpy()\n                    outputs = np.squeeze(outputs[0, 1])\n                    all_outputs.append(outputs)\n                all_outputs = np.asarray(all_outputs)\n\n                # obtain binary segmentation mask\n                seg = np.mean(all_outputs, axis=0)\n                seg[seg >= th] = 1\n                seg[seg < th] = 0\n                seg = np.squeeze(seg)\n                seg = remove_connected_components(seg)\n\n                gt = np.squeeze(gt)\n                brain_mask = np.squeeze(brain_mask)\n\n                # compute reverse mutual information uncertainty map\n                uncs_map = ensemble_uncertainties_classification(np.concatenate(\n                    (np.expand_dims(all_outputs, axis=-1),\n                     np.expand_dims(1. - all_outputs, axis=-1)),\n                    axis=-1))['expected_entropy']\n\n                # compute metrics\n                ndsc += [dice_norm_metric(ground_truth=gt, predictions=seg)]\n                f1 += [lesion_f1_score(ground_truth=gt,\n                                       predictions=seg,\n                                       IoU_threshold=0.5,\n                                       parallel_backend=parallel_backend)]\n                ndsc_aac += [ndsc_aac_metric(ground_truth=gt[brain_mask == 1].flatten(),\n                                             predictions=seg[brain_mask == 1].flatten(),\n                                             uncertainties=uncs_map[brain_mask == 1].flatten(),\n                                             parallel_backend=parallel_backend)]\n\n                # for nervous people\n                if count % 10 == 0:\n                    print(f\"Processed {count}/{len(val_loader)}\")\n\n    ndsc = np.asarray(ndsc) * 100.\n    f1 = np.asarray(f1) * 100.\n    ndsc_aac = np.asarray(ndsc_aac) * 100.\n    results = {}\n    print(f\"nDSC:\\t{np.mean(ndsc):.4f} +- {np.std(ndsc):.4f}\")\n    print(f\"Lesion F1 score:\\t{np.mean(f1):.4f} +- {np.std(f1):.4f}\")\n    print(f\"nDSC R-AUC:\\t{np.mean(ndsc_aac):.4f} +- {np.std(ndsc_aac):.4f}\")\n    \n    results = {\n        \"model_name: \" = \"SegResNet\"\n        \"nDSC\": np.mean(ndsc),\n        \"nDSC std\": np.std(ndsc),\n        \"Lesion F1 score\": np.mean(f1),\n        \"Lesion F1 score std\": np.std(f1),\n        \"nDSC R-AUC\": np.mean(ndsc_aac),\n        \"nDSC R-AUC std\": np.std(ndsc_aac)\n    }\n\n    with open(\"/kaggle/working/SegResNet_results.json\", \"w\") as file:\n        json.dump(results, file)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.950193Z","iopub.execute_input":"2024-05-09T15:17:46.950703Z","iopub.status.idle":"2024-05-09T15:17:46.978577Z","shell.execute_reply.started":"2024-05-09T15:17:46.950671Z","shell.execute_reply":"2024-05-09T15:17:46.977125Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Test The Model","metadata":{}},{"cell_type":"code","source":"path_test_data = '/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/FLAIR'\npath_test_gts = '/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/GroundTruth'\npath_test_bm = '/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/FgMasks'\npath_model = '/kaggle/input/segresnet/SegResNet'\n\ntestSegResNet(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 1, path_model = path_model, num_workers = 4, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:17:46.979856Z","iopub.execute_input":"2024-05-09T15:17:46.980266Z","iopub.status.idle":"2024-05-09T15:49:31.923780Z","shell.execute_reply.started":"2024-05-09T15:17:46.980234Z","shell.execute_reply":"2024-05-09T15:49:31.921584Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Number of validation files: 33\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 3/3 [00:01<00:00,  1.60it/s]\n/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Processed 0/33\nProcessed 10/33\nProcessed 20/33\nProcessed 30/33\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"nDSC:\t71.7560 +- 8.6960\nLesion F1 score:\t35.1109 +- 14.2631\nnDSC R-AUC:\t8.7450 +- 8.1628\n","output_type":"stream"}]}]}