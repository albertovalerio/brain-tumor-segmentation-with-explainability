{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **MS LESION SEGMENTATION TESTING UNET** "]},{"cell_type":"markdown","metadata":{},"source":["Computation of performance metrics (nDSC, lesion F1 score, nDSC R-AUC) for an ensemble of models."]},{"cell_type":"markdown","metadata":{},"source":["## Install Libraries "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:25:33.549360Z","iopub.status.busy":"2024-04-26T10:25:33.548648Z","iopub.status.idle":"2024-04-26T10:25:48.195449Z","shell.execute_reply":"2024-04-26T10:25:48.193773Z","shell.execute_reply.started":"2024-04-26T10:25:33.549324Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting monai==0.9.0\n","  Downloading monai-0.9.0-202206131636-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (2.1.2+cpu)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (1.26.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (2024.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->monai==0.9.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->monai==0.9.0) (1.3.0)\n","Downloading monai-0.9.0-202206131636-py3-none-any.whl (939 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hInstalling collected packages: monai\n","Successfully installed monai-0.9.0\n"]}],"source":["!pip install monai==0.9.0"]},{"cell_type":"markdown","metadata":{},"source":["## Libraries Import"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:25:48.198408Z","iopub.status.busy":"2024-04-26T10:25:48.197979Z","iopub.status.idle":"2024-04-26T10:26:31.009937Z","shell.execute_reply":"2024-04-26T10:26:31.008830Z","shell.execute_reply.started":"2024-04-26T10:25:48.198368Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-26 10:26:21.091407: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-26 10:26:21.091548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-26 10:26:21.275749: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import torch\n","import json\n","import numpy as np\n","import re\n","from joblib import Parallel\n","from monai.inferers import sliding_window_inference\n","from monai.networks.nets import UNet\n","from monai.data import CacheDataset, DataLoader\n","from monai.transforms import (\n","    AddChanneld, Compose, LoadImaged, RandCropByPosNegLabeld,\n","    Spacingd, ToTensord, NormalizeIntensityd, RandFlipd,\n","    RandRotate90d, RandShiftIntensityd, RandAffined, RandSpatialCropd,\n","    RandScaleIntensityd)\n","from glob import glob\n","from scipy import ndimage\n","from functools import partial\n","from collections import Counter\n","from joblib import Parallel, delayed\n","from sklearn import metrics"]},{"cell_type":"markdown","metadata":{},"source":["## Setup Functions"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.011906Z","iopub.status.busy":"2024-04-26T10:26:31.011170Z","iopub.status.idle":"2024-04-26T10:26:31.017112Z","shell.execute_reply":"2024-04-26T10:26:31.016005Z","shell.execute_reply.started":"2024-04-26T10:26:31.011875Z"},"trusted":true},"outputs":[],"source":["def get_default_device():\n","    \"\"\" Set device \"\"\"\n","    if torch.cuda.is_available():\n","        print(\"Got CUDA!\")\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')"]},{"cell_type":"markdown","metadata":{},"source":["### Data Load"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.021012Z","iopub.status.busy":"2024-04-26T10:26:31.020136Z","iopub.status.idle":"2024-04-26T10:26:31.052829Z","shell.execute_reply":"2024-04-26T10:26:31.051335Z","shell.execute_reply.started":"2024-04-26T10:26:31.020968Z"},"trusted":true},"outputs":[],"source":["def remove_connected_components(segmentation, l_min=9):\n","    \"\"\"\n","    Remove all lesions with less or equal amount of voxels than `l_min` from a \n","    binary segmentation mask `segmentation`.\n","    Args:\n","      segmentation: `numpy.ndarray` of shape [H, W, D], with a binary lesions segmentation mask.\n","      l_min:  `int`, minimal amount of voxels in a lesion.\n","    Returns:\n","      Binary lesion segmentation mask (`numpy.ndarray` of shape [H, W, D])\n","      only with connected components that have more than `l_min` voxels.\n","    \"\"\"\n","    labeled_seg, num_labels = ndimage.label(segmentation)\n","    label_list = np.unique(labeled_seg)\n","    num_elements_by_lesion = ndimage.labeled_comprehension(segmentation, labeled_seg, label_list, np.sum, float, 0)\n","\n","    seg2 = np.zeros_like(segmentation)\n","    for i_el, n_el in enumerate(num_elements_by_lesion):\n","        if n_el > l_min:\n","            current_voxels = np.stack(np.where(labeled_seg == i_el), axis=1)\n","            seg2[current_voxels[:, 0],\n","                 current_voxels[:, 1],\n","                 current_voxels[:, 2]] = 1\n","    return seg2"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.055192Z","iopub.status.busy":"2024-04-26T10:26:31.054332Z","iopub.status.idle":"2024-04-26T10:26:31.068478Z","shell.execute_reply":"2024-04-26T10:26:31.067239Z","shell.execute_reply.started":"2024-04-26T10:26:31.055155Z"},"trusted":true},"outputs":[],"source":["def get_val_transforms(keys=[\"image\", \"label\"], image_keys=[\"image\"]):\n","    \"\"\" Get transforms for testing on FLAIR images and ground truth:\n","    - Loads 3D images and masks from Nifti file\n","    - Adds channel dimention\n","    - Applies intensity normalisation to scans\n","    - Converts to torch.Tensor()\n","    \"\"\"\n","    return Compose(\n","        [\n","            LoadImaged(keys=keys),\n","            AddChanneld(keys=keys),\n","            NormalizeIntensityd(keys=image_keys, nonzero=True),\n","            ToTensord(keys=keys),\n","        ]\n","    )"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.070524Z","iopub.status.busy":"2024-04-26T10:26:31.070092Z","iopub.status.idle":"2024-04-26T10:26:31.086995Z","shell.execute_reply":"2024-04-26T10:26:31.085947Z","shell.execute_reply.started":"2024-04-26T10:26:31.070486Z"},"trusted":true},"outputs":[],"source":["def get_val_dataloader(flair_path, gts_path, num_workers, cache_rate=0.1, bm_path=None):\n","    \"\"\"\n","    Get dataloader for validation and testing. Either with or without brain masks.\n","\n","    Args:\n","      flair_path: `str`, path to directory with FLAIR images.\n","      gts_path:  `str`, path to directory with ground truth lesion segmentation \n","                    binary masks images.\n","      num_workers:  `int`,  number of worker threads to use for parallel processing\n","                    of images\n","      cache_rate:  `float` in (0.0, 1.0], percentage of cached data in total.\n","      bm_path:   `None|str`. If `str`, then defines path to directory with\n","                 brain masks. If `None`, dataloader does not return brain masks. \n","    Returns:\n","      monai.data.DataLoader() class object.\n","    \"\"\"\n","    flair = sorted(glob(os.path.join(flair_path, \"*FLAIR_isovox.nii\")),\n","                   key=lambda i: int(re.sub('\\D', '', i)))  # Collect all flair images sorted\n","    segs = sorted(glob(os.path.join(gts_path, \"*_isovox.nii\")),\n","                  key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding ground truths\n","\n","    if bm_path is not None:\n","        bms = sorted(glob(os.path.join(bm_path, \"*isovox_fg_mask.nii\")),\n","                     key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding brain masks\n","\n","        assert len(flair) == len(segs) == len(bms), f\"Some files must be missing: {[len(flair), len(segs), len(bms)]}\"\n","\n","        files = [\n","            {\"image\": fl, \"label\": seg, \"brain_mask\": bm} for fl, seg, bm\n","            in zip(flair, segs, bms)\n","        ]\n","\n","        val_transforms = get_val_transforms(keys=[\"image\", \"label\", \"brain_mask\"])\n","    else:\n","        assert len(flair) == len(segs), f\"Some files must be missing: {[len(flair), len(segs)]}\"\n","\n","        files = [{\"image\": fl, \"label\": seg} for fl, seg in zip(flair, segs)]\n","\n","        val_transforms = get_val_transforms()\n","\n","    print(\"Number of validation files:\", len(files))\n","\n","    ds = CacheDataset(data=files, transform=val_transforms,\n","                      cache_rate=cache_rate, num_workers=num_workers)\n","    return DataLoader(ds, batch_size=1, shuffle=False,\n","                      num_workers=num_workers)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.089009Z","iopub.status.busy":"2024-04-26T10:26:31.088467Z","iopub.status.idle":"2024-04-26T10:26:31.103514Z","shell.execute_reply":"2024-04-26T10:26:31.102384Z","shell.execute_reply.started":"2024-04-26T10:26:31.088969Z"},"trusted":true},"outputs":[],"source":["def dice_norm_metric(ground_truth, predictions):\n","    \"\"\"\n","    Compute Normalised Dice Coefficient (nDSC), \n","    False positive rate (FPR),\n","    False negative rate (FNR) for a single example.\n","    \n","    Args:\n","      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n","                     with shape [H, W, D].\n","      predictions:  `numpy.ndarray`, binary segmentation predictions,\n","                     with shape [H, W, D].\n","    Returns:\n","      Normalised dice coefficient (`float` in [0.0, 1.0]),\n","      False positive rate (`float` in [0.0, 1.0]),\n","      False negative rate (`float` in [0.0, 1.0]),\n","      between `ground_truth` and `predictions`.\n","    \"\"\"\n","\n","    # Reference for normalized DSC\n","    r = 0.001\n","    # Cast to float32 type\n","    gt = ground_truth.astype(\"float32\")\n","    seg = predictions.astype(\"float32\")\n","    im_sum = np.sum(seg) + np.sum(gt)\n","    if im_sum == 0:\n","        return 1.0\n","    else:\n","        if np.sum(gt) == 0:\n","            k = 1.0\n","        else:\n","            k = (1 - r) * np.sum(gt) / (r * (len(gt.flatten()) - np.sum(gt)))\n","        tp = np.sum(seg[gt == 1])\n","        fp = np.sum(seg[gt == 0])\n","        fn = np.sum(gt[seg == 0])\n","        fp_scaled = k * fp\n","        dsc_norm = 2. * tp / (fp_scaled + 2. * tp + fn)\n","        return dsc_norm"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.105385Z","iopub.status.busy":"2024-04-26T10:26:31.105001Z","iopub.status.idle":"2024-04-26T10:26:31.116665Z","shell.execute_reply":"2024-04-26T10:26:31.115726Z","shell.execute_reply.started":"2024-04-26T10:26:31.105355Z"},"trusted":true},"outputs":[],"source":["def ndsc_aac_metric(ground_truth, predictions, uncertainties, parallel_backend=None):\n","    \"\"\"\n","    Compute area above Normalised Dice Coefficient (nDSC) retention curve for \n","    one subject. `ground_truth`, `predictions`, `uncertainties` - are flattened \n","    arrays of correponding 3D maps within the foreground mask only.\n","    \n","    Args:\n","      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n","                     with shape [H * W * D]. \n","      predictions:  `numpy.ndarray`, binary segmentation predictions,\n","                     with shape [H * W * D].\n","      uncertainties:  `numpy.ndarray`, voxel-wise uncertainties,\n","                     with shape [H * W * D].\n","      parallel_backend: `joblib.Parallel`, for parallel computation\n","                     for different retention fractions.\n","    Returns:\n","      nDSC R-AAC (`float` in [0.0, 1.0]).\n","    \"\"\"\n","\n","    def compute_dice_norm(frac_, preds_, gts_, N_):\n","        pos = int(N_ * frac_)\n","        curr_preds = preds if pos == N_ else np.concatenate(\n","            (preds_[:pos], gts_[pos:]))\n","        return dice_norm_metric(gts_, curr_preds)\n","\n","    if parallel_backend is None:\n","        parallel_backend = Parallel(n_jobs=1)\n","\n","    ordering = uncertainties.argsort()\n","    gts = ground_truth[ordering].copy()\n","    preds = predictions[ordering].copy()\n","    N = len(gts)\n","\n","    # # Significant class imbalance means it is important to use logspacing between values\n","    # # so that it is more granular for the higher retention fractions\n","    fracs_retained = np.log(np.arange(200 + 1)[1:])\n","    fracs_retained /= np.amax(fracs_retained)\n","\n","    process = partial(compute_dice_norm, preds_=preds, gts_=gts, N_=N)\n","    dsc_norm_scores = np.asarray(\n","        parallel_backend(delayed(process)(frac)\n","                         for frac in fracs_retained)\n","    )\n","\n","    return 1. - metrics.auc(fracs_retained, dsc_norm_scores)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.118259Z","iopub.status.busy":"2024-04-26T10:26:31.117945Z","iopub.status.idle":"2024-04-26T10:26:31.134077Z","shell.execute_reply":"2024-04-26T10:26:31.132996Z","shell.execute_reply.started":"2024-04-26T10:26:31.118233Z"},"trusted":true},"outputs":[],"source":["def intersection_over_union(mask1, mask2):\n","    \"\"\"\n","    Compute IoU for 2 binary masks.\n","    \n","    Args:\n","      mask1: `numpy.ndarray`, binary mask.\n","      mask2:  `numpy.ndarray`, binary mask of the same shape as `mask1`.\n","    Returns:\n","      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n","    \"\"\"\n","    return np.sum(mask1 * mask2) / np.sum(mask1 + mask2 - mask1 * mask2)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.138349Z","iopub.status.busy":"2024-04-26T10:26:31.137951Z","iopub.status.idle":"2024-04-26T10:26:31.153064Z","shell.execute_reply":"2024-04-26T10:26:31.152104Z","shell.execute_reply.started":"2024-04-26T10:26:31.138318Z"},"trusted":true},"outputs":[],"source":["def lesion_f1_score(ground_truth, predictions, IoU_threshold=0.25, parallel_backend=None):\n","    \"\"\"\n","    Compute lesion-scale F1 score.\n","    \n","    Args:\n","      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n","                     with shape [H, W, D].\n","      predictions:  `numpy.ndarray`, binary segmentation predictions,\n","                     with shape [H, W, D].\n","      IoU_threshold: `float` in [0.0, 1.0], IoU threshold for max IoU between \n","                     predicted and ground truth lesions to classify them as\n","                     TP, FP or FN.\n","      parallel_backend: `joblib.Parallel`, for parallel computation\n","                     for different retention fractions.\n","    Returns:\n","      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n","    \"\"\"\n","\n","    def get_tp_fp(label_pred, mask_multi_pred, mask_multi_gt):\n","        mask_label_pred = (mask_multi_pred == label_pred).astype(int)\n","        all_iou = [0.0]\n","        # iterate only intersections\n","        for int_label_gt in np.unique(mask_multi_gt * mask_label_pred):\n","            if int_label_gt != 0.0:\n","                mask_label_gt = (mask_multi_gt == int_label_gt).astype(int)\n","                all_iou.append(intersection_over_union(\n","                    mask_label_pred, mask_label_gt))\n","        max_iou = max(all_iou)\n","        if max_iou >= IoU_threshold:\n","            return 'tp'\n","        else:\n","            return 'fp'\n","\n","    def get_fn(label_gt, mask_multi_pred, mask_multi_gt):\n","        mask_label_gt = (mask_multi_gt == label_gt).astype(int)\n","        all_iou = [0]\n","        for int_label_pred in np.unique(mask_multi_pred * mask_label_gt):\n","            if int_label_pred != 0.0:\n","                mask_label_pred = (mask_multi_pred ==\n","                                   int_label_pred).astype(int)\n","                all_iou.append(intersection_over_union(\n","                    mask_label_pred, mask_label_gt))\n","        max_iou = max(all_iou)\n","        if max_iou < IoU_threshold:\n","            return 1\n","        else:\n","            return 0\n","\n","    mask_multi_pred_, n_les_pred = ndimage.label(predictions)\n","    mask_multi_gt_, n_les_gt = ndimage.label(ground_truth)\n","\n","    if parallel_backend is None:\n","        parallel_backend = Parallel(n_jobs=1)\n","\n","    process_fp_tp = partial(get_tp_fp, mask_multi_pred=mask_multi_pred_,\n","                            mask_multi_gt=mask_multi_gt_)\n","\n","    tp_fp = parallel_backend(delayed(process_fp_tp)(label_pred)\n","                             for label_pred in np.unique(mask_multi_pred_) if label_pred != 0)\n","    counter = Counter(tp_fp)\n","    tp = float(counter['tp'])\n","    fp = float(counter['fp'])\n","\n","    process_fn = partial(get_fn, mask_multi_pred=mask_multi_pred_,\n","                         mask_multi_gt=mask_multi_gt_)\n","\n","    fn = parallel_backend(delayed(process_fn)(label_gt)\n","                          for label_gt in np.unique(mask_multi_gt_) if label_gt != 0)\n","    fn = float(np.sum(fn))\n","\n","    f1 = 1.0 if tp + 0.5 * (fp + fn) == 0.0 else tp / (tp + 0.5 * (fp + fn))\n","\n","    return f1"]},{"cell_type":"markdown","metadata":{},"source":["### Uncertainty"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.156440Z","iopub.status.busy":"2024-04-26T10:26:31.156058Z","iopub.status.idle":"2024-04-26T10:26:31.172132Z","shell.execute_reply":"2024-04-26T10:26:31.171077Z","shell.execute_reply.started":"2024-04-26T10:26:31.156411Z"},"trusted":true},"outputs":[],"source":["def renyi_entropy_of_expected(probs, alpha=0.8):\n","    \"\"\"\n","    Renyi entropy is a generalised version of Shannon - the two are equivalent for alpha=1\n","    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n","    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n","    \"\"\"\n","    scale = 1. / (1. - alpha)\n","    mean_probs = np.mean(probs, axis=0)\n","    return scale * np.log( np.sum(mean_probs**alpha, axis=-1) )\n","\n","def renyi_expected_entropy(probs, alpha=0.8):\n","    \"\"\"\n","    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n","    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n","    \"\"\"\n","    scale = 1. / (1. - alpha)\n","    return np.mean( scale * np.log( np.sum(probs**alpha, axis=-1) ), axis=0)\n","\n","\n","def entropy_of_expected(probs, epsilon=1e-10):\n","    \"\"\"\n","    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n","    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n","    \"\"\"\n","    mean_probs = np.mean(probs, axis=0)\n","    log_probs = -np.log(mean_probs + epsilon)\n","    return np.sum(mean_probs * log_probs, axis=-1)\n","\n","def expected_entropy(probs, epsilon=1e-10):\n","    \"\"\"\n","    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n","    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n","    \"\"\"\n","    log_probs = -np.log(probs + epsilon)\n","    return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n","\n","\n","def ensemble_uncertainties_classification(probs, epsilon=1e-10):\n","    \"\"\"\n","    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n","    :return: Dictionary of uncertainties\n","    \"\"\"\n","    mean_probs = np.mean(probs, axis=0)\n","    mean_lprobs = np.mean(np.log(probs + epsilon), axis=0)\n","    conf = np.max(mean_probs, axis=-1)\n","\n","    eoe = entropy_of_expected(probs, epsilon)\n","    exe = expected_entropy(probs, epsilon)\n","\n","    mutual_info = eoe - exe\n","\n","    epkl = -np.sum(mean_probs * mean_lprobs, axis=-1) - exe\n","\n","    uncertainty = {'confidence': -1 * conf,\n","                   'entropy_of_expected': eoe,\n","                   'expected_entropy': exe,\n","                   'mutual_information': mutual_info,\n","                   'epkl': epkl,\n","                   'reverse_mutual_information': epkl - mutual_info,\n","                   }\n","\n","    return uncertainty"]},{"cell_type":"markdown","metadata":{},"source":["## Testing Function"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.174041Z","iopub.status.busy":"2024-04-26T10:26:31.173661Z","iopub.status.idle":"2024-04-26T10:26:31.192303Z","shell.execute_reply":"2024-04-26T10:26:31.191230Z","shell.execute_reply.started":"2024-04-26T10:26:31.174007Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Testing function for UNET model.\n","Args:\n","    Data Args:\n","        path_test_data: `str`, path to directory with FLAIR images from test set.\n","        path_test_gts: `str`, path to directory with ground truth lesion segmentation from test set.\n","        path_test_bm: `str`, path to directory with brain masks from test set.\n","    Model Args:\n","        threshold: `float`, threshold for binary segmentation mask.\n","        num_models: `int`, number of models to ensemble.\n","        path_model: `str`, path to directory with trained models.\n","        num_workers: `int`, number of worker threads to use for parallel processing of images.\n","        n_jobs: `int`, number of jobs to run in parallel.\n","\"\"\"\n","def testUNET(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 3, path_model = '', num_workers = 1, n_jobs = 1):\n","    #Setting up the device\n","    device = get_default_device()\n","    torch.multiprocessing.set_sharing_strategy('file_system')\n","    \n","    #Initialise dataloaders\n","    val_loader = get_val_dataloader(flair_path=path_test_data,\n","                                    gts_path=path_test_gts,\n","                                    num_workers=num_workers ,\n","                                    bm_path=path_test_bm)\n","    # Load trained models\n","    K = num_models\n","    models = []\n","    for i in range(K):\n","        models.append(UNet(\n","            spatial_dims=3,\n","            in_channels=1,\n","            out_channels=2,\n","            channels=(32, 64, 128, 256, 512),\n","            strides=(2, 2, 2, 2),\n","            num_res_units=0).to(device)\n","                      )\n","\n","    if(get_default_device() == torch.device('cpu')):\n","        for i, model in enumerate(models):\n","            model.load_state_dict(torch.load(os.path.join(path_model,\n","                                                      f\"seed{i + 1}\",\n","                                                      \"Best_model_finetuning.pth\"),\n","                                                      map_location=torch.device('cpu')))\n","            model.eval()\n","    else :\n","        for i, model in enumerate(models):\n","            model.load_state_dict(torch.load(os.path.join(path_model,\n","                                                      f\"seed{i + 1}\",\n","                                                      \"Best_model_finetuning.pth\")))\n","            model.eval()\n","    \n","    act = torch.nn.Softmax(dim=1)\n","    th = threshold\n","    roi_size = (96, 96, 96)\n","    sw_batch_size = 4\n","\n","    ndsc, f1, ndsc_aac = [], [], []\n","\n","    #Evaluatioin loop\n","    with Parallel(n_jobs= n_jobs) as parallel_backend:\n","        with torch.no_grad():\n","            for count, batch_data in enumerate(val_loader):\n","                inputs, gt, brain_mask = (\n","                    batch_data[\"image\"].to(device),\n","                    batch_data[\"label\"].cpu().numpy(),\n","                    batch_data[\"brain_mask\"].cpu().numpy()\n","                )\n","\n","                # get ensemble predictions\n","                all_outputs = []\n","                for model in models:\n","                    outputs = sliding_window_inference(inputs, roi_size,\n","                                                       sw_batch_size, model,\n","                                                       mode='gaussian')\n","                    outputs = act(outputs).cpu().numpy()\n","                    outputs = np.squeeze(outputs[0, 1])\n","                    all_outputs.append(outputs)\n","                all_outputs = np.asarray(all_outputs)\n","\n","                # obtain binary segmentation mask\n","                seg = np.mean(all_outputs, axis=0)\n","                seg[seg >= th] = 1\n","                seg[seg < th] = 0\n","                seg = np.squeeze(seg)\n","                seg = remove_connected_components(seg)\n","\n","                gt = np.squeeze(gt)\n","                brain_mask = np.squeeze(brain_mask)\n","\n","                # compute reverse mutual information uncertainty map\n","                uncs_map = ensemble_uncertainties_classification(np.concatenate(\n","                    (np.expand_dims(all_outputs, axis=-1),\n","                     np.expand_dims(1. - all_outputs, axis=-1)),\n","                    axis=-1))['expected_entropy']\n","\n","                # compute metrics\n","                ndsc += [dice_norm_metric(ground_truth=gt, predictions=seg)]\n","                f1 += [lesion_f1_score(ground_truth=gt,\n","                                       predictions=seg,\n","                                       IoU_threshold=0.5,\n","                                       parallel_backend=parallel_backend)]\n","                ndsc_aac += [ndsc_aac_metric(ground_truth=gt[brain_mask == 1].flatten(),\n","                                             predictions=seg[brain_mask == 1].flatten(),\n","                                             uncertainties=uncs_map[brain_mask == 1].flatten(),\n","                                             parallel_backend=parallel_backend)]\n","\n","                # for nervous people\n","                if count % 10 == 0:\n","                    print(f\"Processed {count}/{len(val_loader)}\")\n","\n","    ndsc = np.asarray(ndsc) * 100.\n","    f1 = np.asarray(f1) * 100.\n","    ndsc_aac = np.asarray(ndsc_aac) * 100.\n","\n","    print(f\"nDSC:\\t{np.mean(ndsc):.4f} +- {np.std(ndsc):.4f}\")\n","    print(f\"Lesion F1 score:\\t{np.mean(f1):.4f} +- {np.std(f1):.4f}\")\n","    print(f\"nDSC R-AUC:\\t{np.mean(ndsc_aac):.4f} +- {np.std(ndsc_aac):.4f}\")\n","\n","    results = {\n","        \"nDSC\": np.mean(ndsc),\n","        \"nDSC std\": np.std(ndsc),\n","        \"Lesion F1 score\": np.mean(f1),\n","        \"Lesion F1 score std\": np.std(f1),\n","        \"nDSC R-AUC\": np.mean(ndsc_aac),\n","        \"nDSC R-AUC std\": np.std(ndsc_aac)\n","    }\n","\n","    with open(\"results.json\", \"w\") as file:\n","        json.dump(results, file)"]},{"cell_type":"markdown","metadata":{},"source":["## Test The Model"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T10:26:31.194168Z","iopub.status.busy":"2024-04-26T10:26:31.193701Z","iopub.status.idle":"2024-04-26T10:42:18.994270Z","shell.execute_reply":"2024-04-26T10:42:18.992480Z","shell.execute_reply.started":"2024-04-26T10:26:31.194123Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of validation files: 33\n"]},{"name":"stderr","output_type":"stream","text":["Loading dataset: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n","/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["Processed 0/33\n","Processed 10/33\n","Processed 20/33\n","Processed 30/33\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["nDSC:\t69.1128 +- 9.4613\n","Lesion F1 score:\t28.1263 +- 13.0404\n","nDSC R-AUC:\t31.0108 +- 10.9014\n"]}],"source":["path_test_data = '/PathToData/Test/FLAIR'\n","path_test_gts = '/PathToData/Test/GroundTruth'\n","path_test_bm = '/PathToData/Test/FgMasks'\n","path_model = '/PathToModel'\n","\n","testUNET(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 1, path_model = path_model, num_workers = 4, n_jobs = -1)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4826829,"sourceId":8166514,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
